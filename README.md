#  Airflow Data Orchestration

![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.8-blue?style=for-the-badge&logo=apache-airflow&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Python](https://img.shields.io/badge/Python-ETL-yellow?style=for-the-badge)

A production-ready **Apache Airflow** setup designed to orchestrate complex ETL workflows. This project demonstrates how to containerize Airflow using Docker and schedule daily data ingestion pipelines.

##  Architecture
* **Executor:** CeleryExecutor (Scalable worker architecture for parallel tasks).
* **Database:** PostgreSQL (Metadata store).
* **Broker:** Redis (Message broker for task queuing).
* **Containerization:** Fully defined in `docker-compose.yaml`.

##  Project Structure
```bash
â”œâ”€â”€ dags/                # âš¡ Python workflows (DAGs)
â”‚   â””â”€â”€ my_article_pipeline.py # Data extraction logic
â”œâ”€â”€ plugins/             # ğŸ”Œ Custom operators/hooks
â”œâ”€â”€ config/              # âš™ï¸ Airflow configuration overrides
â”œâ”€â”€ docker-compose.yaml  # Infrastructure definition
â””â”€â”€ Dockerfile           # Custom image with dependencies
